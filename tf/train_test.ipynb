{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36176928-9257-4c28-ab19-189b9c1b84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import keras_ocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1c0caa-cf49-4092-accc-bb09138e7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к папке с изображениями и аннотациями\n",
    "data_dir = 'images'\n",
    "annotations_file = os.path.join(data_dir, 'annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb14f09a-c782-4120-861c-7279cf1621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка аннотаций из файла\n",
    "with open(annotations_file, 'r', encoding='utf-8') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b92c502-0283-4388-88ce-e1e3461320cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настраиваем алфавит для распознавания якутского текста\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюяҕҥөһү '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de3d4fa-e9df-4998-9f37-21cbdd1d2e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 03:17:41.929629: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-27 03:17:44.483180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19510 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-09-27 03:17:44.484000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 16858 MB memory:  -> device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:24:00.0, compute capability: 8.6\n",
      "2024-09-27 03:17:44.484677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 5424 MB memory:  -> device: 2, name: NVIDIA RTX A5000, pci bus id: 0000:41:00.0, compute capability: 8.6\n",
      "2024-09-27 03:17:44.485324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22333 MB memory:  -> device: 3, name: NVIDIA RTX A5000, pci bus id: 0000:61:00.0, compute capability: 8.6\n",
      "2024-09-27 03:17:44.485931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 12929 MB memory:  -> device: 4, name: NVIDIA RTX A5000, pci bus id: 0000:a1:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided alphabet does not match pretrained alphabet. Using backbone weights only.\n",
      "Looking for /root/.keras-ocr/crnn_kurapan_notop.h5\n",
      "Downloading /root/.keras-ocr/crnn_kurapan_notop.h5\n"
     ]
    }
   ],
   "source": [
    "# Создаем пайплайн Keras-OCR и настраиваем распознаватель\n",
    "recognizer = keras_ocr.recognition.Recognizer(alphabet=alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b9642df-2aa5-4900-bacf-3bff2547043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для загрузки изображений с помощью OpenCV\n",
    "def load_images_and_annotations(annotations):\n",
    "    images = []\n",
    "    texts = []\n",
    "    for annotation in annotations:\n",
    "        image_path = annotation['image']\n",
    "        text = annotation['text']\n",
    "        \n",
    "        # Загружаем изображение с помощью OpenCV\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Не удалось загрузить изображение: {image_path}\")\n",
    "        \n",
    "        # Конвертация изображения в формат RGB\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img_rgb)\n",
    "        texts.append(text)\n",
    "    \n",
    "    return images, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db8c36fc-55dc-41e0-a716-b3775d345446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем изображения и тексты из аннотаций\n",
    "images, texts = load_images_and_annotations(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f3b2160-0828-45ec-a0b2-f63e15e3b877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Разделяем данные на тренировочные и тестовые\n",
    "split_idx = int(len(images) * 0.8)\n",
    "train_images, test_images = images[:split_idx], images[split_idx:]\n",
    "train_texts, test_texts = texts[:split_idx], texts[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2195a8f8-082a-4442-a241-886db52ec706",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = recognizer.get_batch_generator(\n",
    "    image_generator=zip(train_images, train_texts),\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a69b9b44-f64e-4c57-ad92-890caaa2ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = recognizer.get_batch_generator(\n",
    "    image_generator=zip(test_images, test_texts),\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "961a39d3-8d8a-485b-81c6-56e0fc8242be",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras_ocr.tools' has no attribute 'ctc_batch_cost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m recognizer\u001b[38;5;241m.\u001b[39mtraining_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[43mkeras_ocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_batch_cost\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras_ocr.tools' has no attribute 'ctc_batch_cost'"
     ]
    }
   ],
   "source": [
    "recognizer.training_model.compile(optimizer='adam', loss=keras_ocr.tools.ctc_batch_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f52df8bd-d286-453d-8cef-422f97e73ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_1623/2589838703.py\", line 3, in ctc_loss  *\n        y_pred = tf.transpose(y_pred, [1, 0, 2])  # Транспонируем предсказания для CTC\n\n    ValueError: Dimension must be 2 but is 3 for '{{node ctc_loss/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](model_4/lambda_3/ExpandDims, ctc_loss/transpose/perm)' with input shapes: [?,1], [3].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Можно увеличить количество эпох для лучшего результата\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filefn1nj6_a.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filekjwdtlo8.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__ctc_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtranspose, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred), [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m input_length \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfill, ([ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m1\u001b[39m]], ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m label_length \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnot_equal, (ag__\u001b[38;5;241m.\u001b[39mld(y_true), (\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mint32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_1623/2589838703.py\", line 3, in ctc_loss  *\n        y_pred = tf.transpose(y_pred, [1, 0, 2])  # Транспонируем предсказания для CTC\n\n    ValueError: Dimension must be 2 but is 3 for '{{node ctc_loss/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](model_4/lambda_3/ExpandDims, ctc_loss/transpose/perm)' with input shapes: [?,1], [3].\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "recognizer.training_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    steps_per_epoch=len(train_images) // 8,\n",
    "    validation_steps=len(test_images) // 8,\n",
    "    epochs=10  # Можно увеличить количество эпох для лучшего результата\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1802d5c-a035-44e7-97f4-bfdeb6c1eea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
